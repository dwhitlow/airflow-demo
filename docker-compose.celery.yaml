# Runs Airflow with CeleryExecutor
version: '3'
services:
  airflow:
    build: .
    depends_on:
      - postgres
    ports:
      - "8080:8080"
    volumes:
      - ./airflow.cfg:${AIRFLOW_HOME}/airflow.cfg
      # Sync logs
      - /tmp/airflow-demo/logs:${AIRFLOW_HOME}/logs
    environment:
      # Airflow config variables defined through env vars will override settings in airflow.cfg:
      # https://airflow.apache.org/howto/set-config.html
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql://${DB_USER}:${DB_PASS}@${DB_HOST}:5432/${AIRFLOW_DB_NAME}
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__SCHEDULER__SCHEDULER_HEARTBEAT_SEC=1
      - AIRFLOW__CELERY__BROKER_URL=redis://$CELERY_BROKER_HOST:6379/$CELERY_BROKER_DB
      - AIRFLOW__CELERY__RESULT_BACKEND=redis://$CELERY_BROKER_HOST:6379/$CELERY_BROKER_DB
    command:
      # It's possible to run the scheduler and webserver separately, but running them in the same container simplifies
      # the database initialization.
      ["sh", "-c", "airflow initdb && (airflow scheduler &) && airflow webserver"]
    # Quick and dirty hack to restart airflow if it fails due to postgres not being ready
    restart: on-failure

  # Database used to store airflow state. This is configured through sql_alchemy_conn in config.
  postgres:
    image: postgres:11-alpine
    volumes:
      # Persist postgres data
      - /tmp/airflow-demo/postgres:/var/lib/postgresql/data
    environment:
      - POSTGRES_USER=${DB_USER}
      - POSTGRES_PASSWORD=${DB_PASS}
      - POSTGRES_DB=${AIRFLOW_DB_NAME}

  # The message broker and result backend for Celery. Please note that Airflow recommends some form of persistent store
  # for Celery's result backend. If a persistent store is used for this purpose, celery beat should be
  # configured to periodically clear old results from the DB.
  redis:
    image: redis:5-alpine

  # The containers that will actually execute the tasks scheduled by the airflow container.
  # Normally, Celery workers are started via the `celery` command. `airflow worker` delegates to that command
  # after loading its celery app+config and starting up a logging webserver on the worker.
  celery_worker_1:
    build: .
    depends_on:
      - redis
    volumes:
      # Sync logs
      - /tmp/airflow-demo/logs:${AIRFLOW_HOME}/logs
    environment:
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql://${DB_USER}:${DB_PASS}@${DB_HOST}:5432/${AIRFLOW_DB_NAME}
      - AIRFLOW__CELERY__BROKER_URL=redis://$CELERY_BROKER_HOST:6379/$CELERY_BROKER_DB
      - AIRFLOW__CELERY__RESULT_BACKEND=redis://$CELERY_BROKER_HOST:6379/$CELERY_BROKER_DB
    command:
      ["sh", "-c", "airflow worker"]

  celery_worker_2:
    build: .
    depends_on:
      - redis
    volumes:
      # Sync logs
      - /tmp/airflow-demo/logs:${AIRFLOW_HOME}/logs
    environment:
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql://${DB_USER}:${DB_PASS}@${DB_HOST}:5432/${AIRFLOW_DB_NAME}
      - AIRFLOW__CELERY__BROKER_URL=redis://$CELERY_BROKER_HOST:6379/$CELERY_BROKER_DB
      - AIRFLOW__CELERY__RESULT_BACKEND=redis://$CELERY_BROKER_HOST:6379/$CELERY_BROKER_DB
    command:
      ["sh", "-c", "airflow worker"]

  # Flower is a monitoring tool for celery. It is helpful but not necessary for a celery-based setup.
  # Normally, Flower is started via the `flower` command. `airflow flower` delegates to that command
  # after loading its celery app+config.
  celery_flower:
    build: .
    depends_on:
      - redis
    ports:
      - "5555:5555"
    environment:
      - AIRFLOW__CELERY__BROKER_URL=redis://$CELERY_BROKER_HOST:6379/$CELERY_BROKER_DB
      - AIRFLOW__CELERY__RESULT_BACKEND=redis://$CELERY_BROKER_HOST:6379/$CELERY_BROKER_DB
    command:
      ["sh", "-c", "airflow flower -a \"$${AIRFLOW__CELERY__BROKER_URL}\" --port=5555"]
